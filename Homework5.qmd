---
title: "ST558: Homework 5 - Fitting Models"
author: "Lamia Benyamine"
date: "July 16, 2024"
format: html
editor: visual
---

# *Task 1:* Conceptual Questions

1.  What is the purpose of using cross-validation when fitting a random forest model?
    -   To choose the tuning parameter and split the data.
2.  Describe the bagged tree algorithm.
    -   Bagging is a general method of bootstrap aggregation which gets multiple samples to fit on resample from the data or a fitted model.
3.  What is meant by a general linear model?
    -   A general linear model is using a continuous response variable, but allows for both continuous and categorical predictor variables.
4.  When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?
    -   Adding interactions terms adds more explanatory variables to the model. This could describe the data more and allow for a model with a better fit or it could over fit the model. It allows the model to extend from a simple to a multiple linear regression.
5.  Why do we split our data into a training and test set?
    -   We split the data to train the model based on the training set, and then actually test the prediction of the model on the test set.

# *Task 2:* Fitting Models

This report will be using a Heart Failure Prediction Data set that was created by combining 5 heart data sets with 12 variables. The five data sets used are:

Cleveland: 303 observations\
Hungarian: 294 observations\
Switzerland: 123 observations\
Long Beach VA: 200 observations\
Stalog (Heart) Data Set: 270 observations\
Total: 1190 observations - Duplicated: 272 observations = **Final data set:** 918 observations

## Quick EDA/Data Preparation

Load libraries necessary for this analysis.

```{r libraries, echo = FALSE, message = FALSE, warning = FALSE}
library(readr)
library(tidyr)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(GGally)
library(class)
library(caret)
library(randomForest)
library(tree)
library(rpart.plot)
library(gbm)
```

Read in the data as a tibble and display a few rows.

```{r read csv}
heart_tb <- as_tibble(read_csv("heart.csv", show_col_types = FALSE))

#display a few rows of the data
head(heart_tb)
```

1.  Quickly understand your data. Check on missingness and summarize the data, especially with respect to the relationships of the variables to HeartDisease.

```{r EDA summary 1}
#Determine if there is any missing data in any column
colSums(is.na(heart_tb))
#View a basic data validation summary table
psych::describe(heart_tb)

#Review the rows with 0 values
heart_tb |>
  filter(Cholesterol == 0 | RestingBP == 0)
```

-   There are not any entries with NA, but note there are observations with a RestingBP or Cholesterol equal to 0, which is most likely an error in the data. These entries will be replaced with NA, then imputed with mean values. This way the 0 values do not impact the mean.

2.  Create a new variable that is a factor version of the HeartDisease variable. Remove the ST_Slope variable variable.

```{r}
#create factor variables
heart_tb$HeartDisease = as.factor(heart_tb$HeartDisease)
heart_tb$Sex = as.factor(heart_tb$Sex)
heart_tb$ExerciseAngina = as.factor(heart_tb$ExerciseAngina)
heart_tb$ChestPainType = as.factor(heart_tb$ChestPainType)
heart_tb$RestingECG = as.factor(heart_tb$RestingECG)
heart_tb$FastingBS = as.character(heart_tb$FastingBS)

#remove column
heart_tb = select(heart_tb,-ST_Slope)

#Replace 0 values in cholesterol and restingBP to NA, to then impute values with mean of the respective column
heart_tb[,c('Cholesterol', 'RestingBP')][heart_tb[,c('Cholesterol', 'RestingBP')] == 0] <- NA
heart_tb <- heart_tb |>
  replace_na(list(Cholesterol = mean(heart_tb$Cholesterol, na.rm = TRUE),
                  RestingBP = mean(heart_tb$RestingBP, na.rm = TRUE)))
#View a basic data validation summary table. Note the changes to Cholesterol and RestingBP
psych::describe(heart_tb)

#display a few rows of the data
head(heart_tb)
```

```{r EDA summary 2}
#Numerical Data
heart_tb |>
  select(where(is.numeric), -HeartDisease) |>
    pivot_longer(cols = everything(), names_to = "var", values_to = "value") |>
    ggplot(aes(x = value, fill = var)) +
    facet_wrap(~ var, scales = "free") + #create a plot for each variable in a grid
    geom_density() +
    ggtitle("Distribution Plots for Numeric Variables") +
    guides(fill = "none") + #remove legend
    theme_light()

#Find correlated variables
heart_tb |>
  select(where(is.numeric)) |>
  ggcorr(label = TRUE, palette = "PuOr", name = "Correlation") 

#Review categorical variables
g1 <- heart_tb |> 
  select(HeartDisease, Sex, ChestPainType) |>
  group_by(HeartDisease, Sex, ChestPainType) |>
  summarize(count = n(), .groups = 'drop')

ggplot(data = g1, aes(x = Sex, y =count, fill = HeartDisease)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ ChestPainType) +
  ggtitle("Heart disease by sex and type of chest pain") +
  theme_light()

g2 <- heart_tb |> 
  select(HeartDisease, Sex, RestingECG) |>
  group_by(HeartDisease, Sex, RestingECG) |>
  summarize(count = n(), .groups = 'drop')

ggplot(data = g2, aes(x = Sex, y =count, fill = HeartDisease)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ RestingECG) +
  ggtitle("Heart disease by sex and resting ECG") +
  theme_light()

g3 <- heart_tb |> 
  select(HeartDisease, ChestPainType, RestingECG) |>
  group_by(HeartDisease, ChestPainType, RestingECG) |>
  summarize(count = n(), .groups = 'drop')

ggplot(data = g3, aes(x = RestingECG, y =count, fill = HeartDisease)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ ChestPainType) +
  ggtitle("Heart disease by resting ECG and type of chest pain") +
  theme_light()
```

View basic Linear regression models by Age and Max Heart Rate.

```{r, message = FALSE}
distSum <- heart_tb |>
  group_by(Age) |>
  summarize(propDisease = mean((as.numeric(HeartDisease)-1)), n = n())
ggplot(distSum, aes(x = Age, y = propDisease)) +
  geom_point(stat = "identity", aes(size = n)) + 
  geom_smooth(data = heart_tb, aes(x = Age, y = as.numeric(HeartDisease)-1), method = "lm") +
  ggtitle("Linear Regression Model: Heart diesease by Age") + theme_light()

distSum <- heart_tb |>
  group_by(MaxHR) |>
  summarize(propDisease = mean((as.numeric(HeartDisease)-1)), n = n())
ggplot(distSum, aes(x = MaxHR, y = propDisease)) +
  geom_point(stat = "identity", aes(size = n)) + 
  stat_smooth(data = heart_tb, aes(x = MaxHR, y = as.numeric(HeartDisease)-1), method = "glm", method.args = list(family = "binomial")) +
  ggtitle("Linear Regression Model: Heart diesease by Max Heart Rate") + theme_light()
```

3.  Weâ€™ll be doing a kNN model below to predict whether or not someone has heart disease. To use kNN we generally want to have all numeric predictors. In this case we have some categorical predictors still in our data set: Sex, ExerciseAngina, ChestPainType, and RestingECG.

```{r dummy var, warning=FALSE}
dummies <- dummyVars(HeartDisease ~ ., data = heart_tb)
heart_tb2 <- head(predict(dummies, newdata = heart_tb))
```

## Split Data

Split your data into a training and test set with 70:30 ratio.

```{r split test/train}
#Set seed to get the same training and test set each time
set.seed(10)
heartIndex <- createDataPartition(heart_tb$HeartDisease, p = 0.7, list = FALSE)
head(heartIndex)
#Training set receives 70% of data
heartTrain <- heart_tb[heartIndex, ]
#Testing set receives 30% of data
heartTest <- heart_tb[-heartIndex, ]
```

## kNN

Fit a kNN model and use a 10 fold cross-validation. Train the kNN Model by repeating the 10 fold cross-validation 3 times and set k to be values from 1 to 40.

```{r kNN train}
knnFit <- train(HeartDisease ~  ., 
                              data = heartTrain,
                              method = "knn",
                              preProcess = c("center", "scale"),
                              trControl = trainControl(method = "repeatedcv", 
                                                       number = 10, repeats = 3),
                              tuneGrid = data.frame(k = 1:40)
)
plot(knnFit, main = "Accuracy based on number of k neighbors")
```

> *Based on the highest accuracy, the final value used for the model was k = 3.*

Lastly, check how well your chosen model does on the test set using the confusionMatrix() function.

```{r kNN predict}
knnPred <- confusionMatrix(data = heartTest$HeartDisease, reference = predict(knnFit, newdata = heartTest))
knnPred
```

> *The kNN fit still has a high accuracy after predicting using the test data set based on the confusion matrix.*

## Logistic Regression

Posit three different logistic regression models and fit those models on the training set, using repeated CV as done above.

From the EDA,

```{r}
#Fit all predictor variables
glmFit1 <- train(
  HeartDisease ~ .,
  data = heartTrain,
  method =  "glm",
  family = "binomial",
  preProcess = c("center", "scale"),
  trControl = trainControl(method = "repeatedcv", 
                              number = 10, repeats = 3)
)

#Fit Cholesterol & Age with Resting BP, Age, & Sex
glmFit2 <- train(
  HeartDisease ~ Cholesterol*Age + RestingBP*Age*Sex,
  data = heartTrain,
  method =  "glm",
  family = "binomial",
  preProcess = c("center", "scale"),
  trControl = trainControl(method = "repeatedcv", 
                              number = 10, repeats = 3)
)

#Fit Resting BP, Age, & Sex with Chest Pain Type, Age, & Sex with Max HR and Sex
glmFit3 <- train(
  HeartDisease ~  RestingBP*Age*Sex + ChestPainType*Age*Sex + MaxHR*Sex,
  data = heartTrain,
  method =  "glm",
  family = "binomial",
  preProcess = c("center", "scale"),
  trControl = trainControl(method = "repeatedcv", 
                              number = 10, repeats = 3)
)
#Summary table to determine the best model
fitStats <- data.frame(Fit = c("Fit1", "Fit2", "Fit3"),
                      Accuracy = c(glmFit1$results$Accuracy, glmFit3$results$Accuracy, glmFit3$results$Accuracy),
                      AIC = c(summary(glmFit1)$aic, summary(glmFit2)$aic, summary(glmFit3)$aic))
fitStats
```

Identify your best model and provide a basic summary of it.

-   The best model is Fit1 because it has the lowest AIC value and highest accuracy.

```{r}
#Summary of best model
summary(glmFit1)
```

Lastly, check how well your chosen model does on the test set using the confusionMatrix() function.

```{r}
glmPred <- confusionMatrix(data = heartTest$HeartDisease, reference = predict(glmFit1, newdata = heartTest))
glmPred
```

-   Fit1 still has a high accuracy after predicting using the test data set based on the confusion matrix.

## Tree Models

In this section weâ€™ll fit a few different tree based models in a similar way as above! Choose your own variables of interest. Use repeated 10 fold CV to select a best:

-   classification tree model (use method = rpart: tuning parameter is cp, use values 0, 0.001, 0.002, . . . , 0.1). Center and scale the data.

```{r}
#Tree model using MaxHR, RestingBP, Age, and Cholesterol as predictors
treeFit1 <- train(HeartDisease ~ ChestPainType + ExerciseAngina, 
                  data = heartTrain,
                  method = "rpart",
                  preProcess = c("center", "scale"),
                  trControl = trainControl(method = "repeatedcv", 
                              number = 10, repeats = 3)
)
#Print the summary and view the cp values to determine the optimal size tree
treeFit1$results
treeFit1$bestTune

#Plot the classification tree
rpart.plot(treeFit1$finalModel, main="Classification Tree for Heart Disease")
```

> *Based on the lowest cp values and highest accuracy, the final used for the model was cp = 0.02787.*

-   a random forest (use method = rf: tuning parameter is mtry, use values of 1, 2, . . . , \# of predictors (bagging is a special case here m=p!) There are 4 classification variables so m = 4. Center and scale the data.

```{r, warning = FALSE}
#set the bootstrap sample for test and train data
set.seed(10)
#80% in training set
train <- sample(1:nrow(heart_tb), size = nrow(heart_tb)*0.8)
#20% in test set
test <- dplyr::setdiff(1:nrow(heart_tb), train)

heartTrain2 <- heart_tb[train, ]
heartTest2 <- heart_tb[test, ]

#Get the bagged model fit
treeFit2 <- train(HeartDisease ~., data = heartTrain2,
                  method = "rf",
                  preProcess = c("center", "scale"),
                  trControl = trainControl(method = "repeatedcv", 
                              number = 10, repeats = 3),
                  tuneGrid = data.frame(mtry = 1:4)
)
#Print the summary and best option
treeFit2$results
treeFit2$bestTune
```

> *Based on the highest accuracy, the final value used for the model was mtry = 2.*

-   a boosted tree (use method = gbm: tuning parameters are n.trees, interaction.depth, shrinkage, and n.minobsinnode, use all combinations of n.trees of 25, 50, 100, and 200, interaction.depth of 1, 2, 3, shrinkage = 0.1, and nminobsinnode = 10; Hint: use expand.grid() to create your data frame for tuneGrid and verbose = FALSE limits the output produced. Center and scale the data.

```{r boosted}
ntree <- c(25, 50, 100, 200)
intDep <- c(1, 2, 3)
shrink <- c(0.1)
nmin <- c(10)
gbmGrid <- expand.grid(n.trees = ntree, interaction.depth = intDep, shrinkage = shrink, n.minobsinnode = nmin)

treeFit3 <- train(HeartDisease ~ ., data = heartTrain2, method = "gbm",
                distribution = "bernoulli", 
                preProcess = c("center", "scale"),
                tuneGrid = gbmGrid, 
                trControl = trainControl(method = "repeatedcv", 
                              number = 10, repeats = 3),
                verbose = FALSE)
#Print the summary and best option
treeFit3$results
treeFit3$bestTune
```

> *Based on the highest accuracy, the final values used for the model were n.trees = 100, interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode = 10.*

Lastly, check how well each of your chosen models do on the test set using the confusionMatrix() function.

```{r}
rpartPred <- confusionMatrix(data = heartTest2$HeartDisease, reference = predict(treeFit1, newdata = heartTest2))
rfPred <- confusionMatrix(data = heartTest2$HeartDisease, reference = predict(treeFit2, newdata = heartTest2))
boostPred <- confusionMatrix(data = heartTest2$HeartDisease, reference = predict(treeFit3, newdata = heartTest2))

#Combine the statistics to view in a table
fitTreeStats <- data.frame(rpartPred$overall, rfPred$overall, boostPred$overall)
fitTreeStats
```

> *Based on the highest accuracy, the best model is the random forest model.*

## Wrap Up

Which model overall did the best job (in terms of accuracy) on the test set?

-   Here is reminder of the accuracy values for the top model in each section.

```{r}
#Combine the statistic values for the top models
statCombine <- data.frame(knnPred$overall, glmPred$overall, rfPred$overall)
#only display accuracy levels
statCombine[1,]
```

> *Based on the highest accuracy, the random forest model did the best job fitting and predicting the data*
