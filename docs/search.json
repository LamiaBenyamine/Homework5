[
  {
    "objectID": "Homework5.html",
    "href": "Homework5.html",
    "title": "ST558: Homework 5 - Fitting Models",
    "section": "",
    "text": "What is the purpose of using cross-validation when fitting a random forest model?\n\nTo choose the tuning parameter and split the data.\n\nDescribe the bagged tree algorithm.\n\nBagging is a general method of bootstrap aggregation which gets multiple samples to fit on resample from the data or a fitted model.\n\nWhat is meant by a general linear model?\n\nA general linear model is using a continuous response variable, but allows for both continuous and categorical predictor variables.\n\nWhen fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?\n\nAdding interactions terms adds more explanatory variables to the model. This could describe the data more and allow for a model with a better fit or it could over fit the model. It allows the model to extend from a simple to a multiple linear regression.\n\nWhy do we split our data into a training and test set?\n\nWe split the data to train the model based on the training set, and then actually test the prediction of the model on the test set."
  },
  {
    "objectID": "Homework5.html#quick-edadata-preparation",
    "href": "Homework5.html#quick-edadata-preparation",
    "title": "ST558: Homework 5 - Fitting Models",
    "section": "Quick EDA/Data Preparation",
    "text": "Quick EDA/Data Preparation\nLoad libraries necessary for this analysis.\n\nlibrary(readr)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(GGally)\nlibrary(class)\nlibrary(caret)\nlibrary(randomForest)\nlibrary(tree)\nlibrary(rpart.plot)\nlibrary(gbm)\n\nRead in the data as a tibble and display a few rows.\n\nheart_tb &lt;- as_tibble(read_csv(\"heart.csv\", show_col_types = FALSE))\n\n#display a few rows of the data\nhead(heart_tb)\n\n# A tibble: 6 × 12\n    Age Sex   ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1    40 M     ATA                 140         289         0 Normal       172\n2    49 F     NAP                 160         180         0 Normal       156\n3    37 M     ATA                 130         283         0 ST            98\n4    48 F     ASY                 138         214         0 Normal       108\n5    54 M     NAP                 150         195         0 Normal       122\n6    39 M     NAP                 120         339         0 Normal       170\n# ℹ 4 more variables: ExerciseAngina &lt;chr&gt;, Oldpeak &lt;dbl&gt;, ST_Slope &lt;chr&gt;,\n#   HeartDisease &lt;dbl&gt;\n\n\n\nQuickly understand your data. Check on missingness and summarize the data, especially with respect to the relationships of the variables to HeartDisease.\n\n\n#Determine if there is any missing data in any column\ncolSums(is.na(heart_tb))\n\n           Age            Sex  ChestPainType      RestingBP    Cholesterol \n             0              0              0              0              0 \n     FastingBS     RestingECG          MaxHR ExerciseAngina        Oldpeak \n             0              0              0              0              0 \n      ST_Slope   HeartDisease \n             0              0 \n\n#View a basic data validation summary table\npsych::describe(heart_tb, skew = FALSE, omit = TRUE)\n\n             vars   n   mean     sd median  min   max range   se\nAge             1 918  53.51   9.43   54.0 28.0  77.0  49.0 0.31\nRestingBP       4 918 132.40  18.51  130.0  0.0 200.0 200.0 0.61\nCholesterol     5 918 198.80 109.38  223.0  0.0 603.0 603.0 3.61\nFastingBS       6 918   0.23   0.42    0.0  0.0   1.0   1.0 0.01\nMaxHR           8 918 136.81  25.46  138.0 60.0 202.0 142.0 0.84\nOldpeak        10 918   0.89   1.07    0.6 -2.6   6.2   8.8 0.04\nHeartDisease   12 918   0.55   0.50    1.0  0.0   1.0   1.0 0.02\n\n#Review the rows with 0 values\nheart_tb |&gt;\n  filter(Cholesterol == 0 | RestingBP == 0)\n\n# A tibble: 172 × 12\n     Age Sex   ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1    65 M     ASY                 115           0         0 Normal        93\n 2    32 M     TA                   95           0         1 Normal       127\n 3    61 M     ASY                 105           0         1 Normal       110\n 4    50 M     ASY                 145           0         1 Normal       139\n 5    57 M     ASY                 110           0         1 ST           131\n 6    51 M     ASY                 110           0         1 Normal        92\n 7    47 M     ASY                 110           0         1 ST           149\n 8    60 M     ASY                 160           0         1 Normal       149\n 9    55 M     ATA                 140           0         0 ST           150\n10    53 M     ASY                 125           0         1 Normal       120\n# ℹ 162 more rows\n# ℹ 4 more variables: ExerciseAngina &lt;chr&gt;, Oldpeak &lt;dbl&gt;, ST_Slope &lt;chr&gt;,\n#   HeartDisease &lt;dbl&gt;\n\n\n\nThere are not any entries with NA, but note there are observations with a RestingBP or Cholesterol equal to 0, which is most likely an error in the data. These entries will be replaced with NA, then imputed with mean values. This way the 0 values do not impact the mean.\n\n\nCreate a new variable that is a factor version of the HeartDisease variable. Remove the ST_Slope variable variable.\n\n\n#create factor variables\nheart_tb$HeartDisease = as.factor(heart_tb$HeartDisease)\nheart_tb$Sex = as.factor(heart_tb$Sex)\nheart_tb$ExerciseAngina = as.factor(heart_tb$ExerciseAngina)\nheart_tb$ChestPainType = as.factor(heart_tb$ChestPainType)\nheart_tb$RestingECG = as.factor(heart_tb$RestingECG)\nheart_tb$FastingBS = as.character(heart_tb$FastingBS)\n\n#remove column\nheart_tb = select(heart_tb,-ST_Slope)\n\n#Replace 0 values in cholesterol and restingBP to NA, to then impute values with mean of the respective column\nheart_tb[,c('Cholesterol', 'RestingBP')][heart_tb[,c('Cholesterol', 'RestingBP')] == 0] &lt;- NA\nheart_tb &lt;- heart_tb |&gt;\n  replace_na(list(Cholesterol = mean(heart_tb$Cholesterol, na.rm = TRUE),\n                  RestingBP = mean(heart_tb$RestingBP, na.rm = TRUE)))\n#View a basic data validation summary table. Note the changes to Cholesterol and RestingBP\npsych::describe(heart_tb, skew = FALSE, omit = TRUE)\n\n            vars   n   mean    sd median  min   max range   se\nAge            1 918  53.51  9.43  54.00 28.0  77.0  49.0 0.31\nRestingBP      4 918 132.54 17.99 130.00 80.0 200.0 120.0 0.59\nCholesterol    5 918 244.64 53.32 244.64 85.0 603.0 518.0 1.76\nMaxHR          8 918 136.81 25.46 138.00 60.0 202.0 142.0 0.84\nOldpeak       10 918   0.89  1.07   0.60 -2.6   6.2   8.8 0.04\n\n#display a few rows of the data\nhead(heart_tb)\n\n# A tibble: 6 × 11\n    Age Sex   ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n  &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;fct&gt;      &lt;dbl&gt;\n1    40 M     ATA                 140         289 0         Normal       172\n2    49 F     NAP                 160         180 0         Normal       156\n3    37 M     ATA                 130         283 0         ST            98\n4    48 F     ASY                 138         214 0         Normal       108\n5    54 M     NAP                 150         195 0         Normal       122\n6    39 M     NAP                 120         339 0         Normal       170\n# ℹ 3 more variables: ExerciseAngina &lt;fct&gt;, Oldpeak &lt;dbl&gt;, HeartDisease &lt;fct&gt;\n\n\nCreate plots to visualize the data.\n\n#Numerical Data\nheart_tb |&gt;\n  select(where(is.numeric), -HeartDisease) |&gt;\n    pivot_longer(cols = everything(), names_to = \"var\", values_to = \"value\") |&gt;\n    ggplot(aes(x = value, fill = var)) +\n    facet_wrap(~ var, scales = \"free\") + #create a plot for each variable in a grid\n    geom_density() +\n    ggtitle(\"Distribution Plots for Numeric Variables\") +\n    guides(fill = \"none\") + #remove legend\n    theme_light()\n\n\n\n\n\n\n\n#Find correlated variables\nheart_tb |&gt;\n  select(where(is.numeric)) |&gt;\n  ggcorr(label = TRUE, palette = \"PuOr\", name = \"Correlation\") \n\n\n\n\n\n\n\n#Review categorical variables\ng1 &lt;- heart_tb |&gt; \n  select(HeartDisease, Sex, ChestPainType) |&gt;\n  group_by(HeartDisease, Sex, ChestPainType) |&gt;\n  summarize(count = n(), .groups = 'drop')\n\nggplot(data = g1, aes(x = Sex, y =count, fill = HeartDisease)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_wrap(~ ChestPainType) +\n  ggtitle(\"Heart disease by sex and type of chest pain\") +\n  theme_light()\n\n\n\n\n\n\n\ng2 &lt;- heart_tb |&gt; \n  select(HeartDisease, Sex, RestingECG) |&gt;\n  group_by(HeartDisease, Sex, RestingECG) |&gt;\n  summarize(count = n(), .groups = 'drop')\n\nggplot(data = g2, aes(x = Sex, y =count, fill = HeartDisease)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_wrap(~ RestingECG) +\n  ggtitle(\"Heart disease by sex and resting ECG\") +\n  theme_light()\n\n\n\n\n\n\n\ng3 &lt;- heart_tb |&gt; \n  select(HeartDisease, ChestPainType, RestingECG) |&gt;\n  group_by(HeartDisease, ChestPainType, RestingECG) |&gt;\n  summarize(count = n(), .groups = 'drop')\n\nggplot(data = g3, aes(x = RestingECG, y =count, fill = HeartDisease)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_wrap(~ ChestPainType) +\n  ggtitle(\"Heart disease by resting ECG and type of chest pain\") +\n  theme_light()\n\n\n\n\n\n\n\n\n\nReview EDA\nFrom the EDA, there aren’t any highly correlated numerical values. The categorical variables shows a large amount of heart disease male patients with asymptomatic chest pain. There are also higher instances of heart disease in male patients with a normal resting electrocardiogram. We will take these observations and use them to create prediction models.\nView basic Linear regression models by Age and Max Heart Rate.\n\ndistSum &lt;- heart_tb |&gt;\n  group_by(Age) |&gt;\n  summarize(propDisease = mean((as.numeric(HeartDisease)-1)), n = n())\nggplot(distSum, aes(x = Age, y = propDisease)) +\n  geom_point(stat = \"identity\", aes(size = n)) + \n  geom_smooth(data = heart_tb, aes(x = Age, y = as.numeric(HeartDisease)-1), method = \"lm\") +\n  ggtitle(\"Linear Regression Model: Heart diesease by Age\") + theme_light()\n\n\n\n\n\n\n\ndistSum &lt;- heart_tb |&gt;\n  group_by(MaxHR) |&gt;\n  summarize(propDisease = mean((as.numeric(HeartDisease)-1)), n = n())\nggplot(distSum, aes(x = MaxHR, y = propDisease)) +\n  geom_point(stat = \"identity\", aes(size = n)) + \n  stat_smooth(data = heart_tb, aes(x = MaxHR, y = as.numeric(HeartDisease)-1), method = \"glm\", method.args = list(family = \"binomial\")) +\n  ggtitle(\"Linear Regression Model: Heart diesease by Max Heart Rate\") + theme_light()\n\n\n\n\n\n\n\n\n\nWe’ll be doing a kNN model below to predict whether or not someone has heart disease. To use kNN we generally want to have all numeric predictors. In this case we have some categorical predictors still in our data set: Sex, ExerciseAngina, ChestPainType, and RestingECG.\n\nCreate dummy columns corresponding to the values of these four variables for use in our kNN fit\n\ndummies &lt;- dummyVars(HeartDisease ~ ., data = heart_tb)\nheart_tb2 &lt;- head(predict(dummies, newdata = heart_tb))"
  },
  {
    "objectID": "Homework5.html#split-data",
    "href": "Homework5.html#split-data",
    "title": "ST558: Homework 5 - Fitting Models",
    "section": "Split Data",
    "text": "Split Data\nSplit your data into a training and test set with 70:30 ratio.\n\n#Set seed to get the same training and test set each time\nset.seed(10)\nheartIndex &lt;- createDataPartition(heart_tb$HeartDisease, p = 0.7, list = FALSE)\nhead(heartIndex)\n\n     Resample1\n[1,]         2\n[2,]         4\n[3,]         6\n[4,]         9\n[5,]        11\n[6,]        12\n\n#Training set receives 70% of data\nheartTrain &lt;- heart_tb[heartIndex, ]\n#Testing set receives 30% of data\nheartTest &lt;- heart_tb[-heartIndex, ]"
  },
  {
    "objectID": "Homework5.html#knn",
    "href": "Homework5.html#knn",
    "title": "ST558: Homework 5 - Fitting Models",
    "section": "kNN",
    "text": "kNN\nFit a kNN model and use a 10 fold cross-validation. Train the kNN Model by repeating the 10 fold cross-validation 3 times and set k to be values from 1 to 40.\n\nknnFit &lt;- train(HeartDisease ~  ., \n                              data = heartTrain,\n                              method = \"knn\",\n                              preProcess = c(\"center\", \"scale\"),\n                              trControl = trainControl(method = \"repeatedcv\", \n                                                       number = 10, repeats = 3),\n                              tuneGrid = data.frame(k = 1:40)\n)\nplot(knnFit, main = \"Accuracy based on number of k neighbors\")\n\n\n\n\n\n\n\n\n\nBased on the highest accuracy, the final value used for the model was k = 3.\n\nLastly, check how well your chosen model does on the test set using the confusionMatrix() function.\n\nknnPred &lt;- confusionMatrix(data = heartTest$HeartDisease, reference = predict(knnFit, newdata = heartTest))\nknnPred\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 103  20\n         1  31 121\n                                          \n               Accuracy : 0.8145          \n                 95% CI : (0.7635, 0.8587)\n    No Information Rate : 0.5127          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.6281          \n                                          \n Mcnemar's Test P-Value : 0.1614          \n                                          \n            Sensitivity : 0.7687          \n            Specificity : 0.8582          \n         Pos Pred Value : 0.8374          \n         Neg Pred Value : 0.7961          \n             Prevalence : 0.4873          \n         Detection Rate : 0.3745          \n   Detection Prevalence : 0.4473          \n      Balanced Accuracy : 0.8134          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\nThe kNN fit still has a high accuracy after predicting using the test data set based on the confusion matrix."
  },
  {
    "objectID": "Homework5.html#logistic-regression",
    "href": "Homework5.html#logistic-regression",
    "title": "ST558: Homework 5 - Fitting Models",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nPosit three different logistic regression models and fit those models on the training set, using repeated CV as done above.\n\n#Fit all predictor variables\nglmFit1 &lt;- train(\n  HeartDisease ~ .,\n  data = heartTrain,\n  method =  \"glm\",\n  family = \"binomial\",\n  preProcess = c(\"center\", \"scale\"),\n  trControl = trainControl(method = \"repeatedcv\", \n                              number = 10, repeats = 3)\n)\n\n#Fit Cholesterol & Age with Resting BP, Age, & Sex\nglmFit2 &lt;- train(\n  HeartDisease ~ Cholesterol*Age + RestingBP*Age*Sex,\n  data = heartTrain,\n  method =  \"glm\",\n  family = \"binomial\",\n  preProcess = c(\"center\", \"scale\"),\n  trControl = trainControl(method = \"repeatedcv\", \n                              number = 10, repeats = 3)\n)\n\n#Fit Resting BP, Age, & Sex with Chest Pain Type, Age, & Sex with Max HR and Sex\nglmFit3 &lt;- train(\n  HeartDisease ~  RestingBP*Age*Sex + ChestPainType*Age*Sex + MaxHR*Sex,\n  data = heartTrain,\n  method =  \"glm\",\n  family = \"binomial\",\n  preProcess = c(\"center\", \"scale\"),\n  trControl = trainControl(method = \"repeatedcv\", \n                              number = 10, repeats = 3)\n)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n#Summary table to determine the best model\nfitStats &lt;- data.frame(glmFit = c(\"Fit1\", \"Fit2\", \"Fit3\"),\n                      Accuracy = c(glmFit1$results$Accuracy, glmFit3$results$Accuracy, glmFit3$results$Accuracy),\n                      AIC = c(summary(glmFit1)$aic, summary(glmFit2)$aic, summary(glmFit3)$aic))\nfitStats\n\n  glmFit  Accuracy      AIC\n1   Fit1 0.8256922 523.7054\n2   Fit2 0.7791966 792.9204\n3   Fit3 0.7791966 621.3487\n\n\nIdentify your best model and provide a basic summary of it.\n\nThe best model is Fit1 because it has the lowest AIC value and highest accuracy.\n\n\n#Summary of best model\nsummary(glmFit1)\n\n\nCall:\nNULL\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       0.39757    0.11945   3.328 0.000873 ***\nAge               0.01029    0.13337   0.077 0.938486    \nSexM              0.50747    0.12133   4.183 2.88e-05 ***\nChestPainTypeATA -0.81868    0.13367  -6.124 9.10e-10 ***\nChestPainTypeNAP -0.61556    0.11338  -5.429 5.66e-08 ***\nChestPainTypeTA  -0.32594    0.10336  -3.154 0.001613 ** \nRestingBP         0.02014    0.11838   0.170 0.864893    \nCholesterol       0.23964    0.11646   2.058 0.039628 *  \nFastingBS1        0.63031    0.12420   5.075 3.87e-07 ***\nRestingECGNormal  0.19742    0.14713   1.342 0.179642    \nRestingECGST      0.06938    0.15017   0.462 0.644088    \nMaxHR            -0.46330    0.12868  -3.600 0.000318 ***\nExerciseAnginaY   0.62213    0.12750   4.879 1.06e-06 ***\nOldpeak           0.66017    0.13306   4.962 6.99e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 883.97  on 642  degrees of freedom\nResidual deviance: 495.71  on 629  degrees of freedom\nAIC: 523.71\n\nNumber of Fisher Scoring iterations: 5\n\n\nLastly, check how well your chosen model does on the test set using the confusionMatrix() function.\n\nglmPred &lt;- confusionMatrix(data = heartTest$HeartDisease, reference = predict(glmFit1, newdata = heartTest))\nglmPred\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0  99  24\n         1  34 118\n                                          \n               Accuracy : 0.7891          \n                 95% CI : (0.7361, 0.8358)\n    No Information Rate : 0.5164          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.5767          \n                                          \n Mcnemar's Test P-Value : 0.2373          \n                                          \n            Sensitivity : 0.7444          \n            Specificity : 0.8310          \n         Pos Pred Value : 0.8049          \n         Neg Pred Value : 0.7763          \n             Prevalence : 0.4836          \n         Detection Rate : 0.3600          \n   Detection Prevalence : 0.4473          \n      Balanced Accuracy : 0.7877          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\nFit1 still has a high accuracy after predicting using the test data set based on the confusion matrix."
  },
  {
    "objectID": "Homework5.html#tree-models",
    "href": "Homework5.html#tree-models",
    "title": "ST558: Homework 5 - Fitting Models",
    "section": "Tree Models",
    "text": "Tree Models\nIn this section we’ll fit a few different tree based models in a similar way as above! Choose your own variables of interest. Use repeated 10 fold CV to select a best:\n\nclassification tree model (use method = rpart: tuning parameter is cp, use values 0, 0.001, 0.002, . . . , 0.1). Center and scale the data.\n\n\n#Tree model using MaxHR, RestingBP, Age, and Cholesterol as predictors\ntreeFit1 &lt;- train(HeartDisease ~ ChestPainType + ExerciseAngina, \n                  data = heartTrain,\n                  method = \"rpart\",\n                  preProcess = c(\"center\", \"scale\"),\n                  trControl = trainControl(method = \"repeatedcv\", \n                              number = 10, repeats = 3)\n)\n#Print the summary and view the cp values to determine the optimal size tree\ntreeFit1$results\n\n          cp  Accuracy     Kappa AccuracySD    KappaSD\n1 0.02787456 0.7765849 0.5377443 0.04731890 0.09882318\n2 0.05226481 0.7410157 0.4773720 0.05337881 0.10154908\n3 0.40418118 0.6322863 0.2255625 0.07168055 0.20374938\n\ntreeFit1$bestTune\n\n          cp\n1 0.02787456\n\n#Plot the classification tree\nrpart.plot(treeFit1$finalModel, main=\"Classification Tree for Heart Disease\")\n\n\n\n\n\n\n\n\n\nBased on the lowest cp values and highest accuracy, the final used for the model was cp = 0.02787.\n\n\na random forest (use method = rf: tuning parameter is mtry, use values of 1, 2, . . . , # of predictors (bagging is a special case here m=p!) There are 4 classification variables so m = 4. Center and scale the data.\n\n\n#set the bootstrap sample for test and train data\nset.seed(10)\n#80% in training set\ntrain &lt;- sample(1:nrow(heart_tb), size = nrow(heart_tb)*0.8)\n#20% in test set\ntest &lt;- dplyr::setdiff(1:nrow(heart_tb), train)\n\nheartTrain2 &lt;- heart_tb[train, ]\nheartTest2 &lt;- heart_tb[test, ]\n\n#Get the bagged model fit\ntreeFit2 &lt;- train(HeartDisease ~., data = heartTrain2,\n                  method = \"rf\",\n                  preProcess = c(\"center\", \"scale\"),\n                  trControl = trainControl(method = \"repeatedcv\", \n                              number = 10, repeats = 3),\n                  tuneGrid = data.frame(mtry = 1:4)\n)\n#Print the summary and best option\ntreeFit2$results\n\n  mtry  Accuracy     Kappa AccuracySD    KappaSD\n1    1 0.8251327 0.6451461 0.03698403 0.07488819\n2    2 0.8306121 0.6572118 0.03372878 0.06816141\n3    3 0.8278601 0.6514516 0.03204837 0.06478203\n4    4 0.8287610 0.6530937 0.03290348 0.06716838\n\ntreeFit2$bestTune\n\n  mtry\n2    2\n\n\n\nBased on the highest accuracy, the final value used for the model was mtry = 2.\n\n\na boosted tree (use method = gbm: tuning parameters are n.trees, interaction.depth, shrinkage, and n.minobsinnode, use all combinations of n.trees of 25, 50, 100, and 200, interaction.depth of 1, 2, 3, shrinkage = 0.1, and nminobsinnode = 10; Hint: use expand.grid() to create your data frame for tuneGrid and verbose = FALSE limits the output produced. Center and scale the data.\n\n\nntree &lt;- c(25, 50, 100, 200)\nintDep &lt;- c(1, 2, 3)\nshrink &lt;- c(0.1)\nnmin &lt;- c(10)\ngbmGrid &lt;- expand.grid(n.trees = ntree, interaction.depth = intDep, shrinkage = shrink, n.minobsinnode = nmin)\n\ntreeFit3 &lt;- train(HeartDisease ~ ., data = heartTrain2, method = \"gbm\",\n                distribution = \"bernoulli\", \n                preProcess = c(\"center\", \"scale\"),\n                tuneGrid = gbmGrid, \n                trControl = trainControl(method = \"repeatedcv\", \n                              number = 10, repeats = 3),\n                verbose = FALSE)\n#Print the summary and best option\ntreeFit3$results\n\n   shrinkage interaction.depth n.minobsinnode n.trees  Accuracy     Kappa\n1        0.1                 1             10      25 0.7992622 0.5951914\n5        0.1                 2             10      25 0.8183913 0.6317375\n9        0.1                 3             10      25 0.8183051 0.6319800\n2        0.1                 1             10      50 0.8174285 0.6311021\n6        0.1                 2             10      50 0.8301220 0.6567525\n10       0.1                 3             10      50 0.8260246 0.6482344\n3        0.1                 1             10     100 0.8269870 0.6505639\n7        0.1                 2             10     100 0.8301712 0.6567801\n11       0.1                 3             10     100 0.8328804 0.6620078\n4        0.1                 1             10     200 0.8269809 0.6505910\n8        0.1                 2             10     200 0.8301834 0.6567984\n12       0.1                 3             10     200 0.8215261 0.6386962\n   AccuracySD    KappaSD\n1  0.04056833 0.08091256\n5  0.04524041 0.09124623\n9  0.04315631 0.08767623\n2  0.04278554 0.08498354\n6  0.04394229 0.08781457\n10 0.03898347 0.07780994\n3  0.04457995 0.08916791\n7  0.04552158 0.09130976\n11 0.04160932 0.08374196\n4  0.04291375 0.08634218\n8  0.04340608 0.08715589\n12 0.04538874 0.09101301\n\ntreeFit3$bestTune\n\n   n.trees interaction.depth shrinkage n.minobsinnode\n11     100                 3       0.1             10\n\n\n\nBased on the highest accuracy, the final values used for the model were n.trees = 100, interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode = 10.\n\nLastly, check how well each of your chosen models do on the test set using the confusionMatrix() function.\n\nrpartPred &lt;- confusionMatrix(data = heartTest2$HeartDisease, reference = predict(treeFit1, newdata = heartTest2))\nrfPred &lt;- confusionMatrix(data = heartTest2$HeartDisease, reference = predict(treeFit2, newdata = heartTest2))\nboostPred &lt;- confusionMatrix(data = heartTest2$HeartDisease, reference = predict(treeFit3, newdata = heartTest2))\n\n#Combine the statistics to view in a table\nfitTreeStats &lt;- data.frame(rpartPred$overall, rfPred$overall, boostPred$overall)\nfitTreeStats\n\n               rpartPred.overall rfPred.overall boostPred.overall\nAccuracy            0.7826086957   8.043478e-01      8.260870e-01\nKappa               0.5405169185   6.007233e-01      6.428485e-01\nAccuracyLower       0.7159592304   7.395721e-01      7.634499e-01\nAccuracyUpper       0.8399193511   8.590451e-01      8.778980e-01\nAccuracyNull        0.6902173913   5.706522e-01      5.923913e-01\nAccuracyPValue      0.0034267352   1.815554e-11      8.488525e-12\nMcnemarPValue       0.0008989128   1.000000e+00      5.958831e-01\n\n\n\nBased on the highest accuracy, the best model is the random forest model."
  },
  {
    "objectID": "Homework5.html#wrap-up",
    "href": "Homework5.html#wrap-up",
    "title": "ST558: Homework 5 - Fitting Models",
    "section": "Wrap Up",
    "text": "Wrap Up\nWhich model overall did the best job (in terms of accuracy) on the test set?\n\nHere is reminder of the accuracy values for the top model in each section.\n\n\n#Combine the statistic values for the top models\nstatCombine &lt;- data.frame(knnPred$overall, glmPred$overall, rfPred$overall)\n#only display accuracy levels\nstatCombine[1,]\n\n         knnPred.overall glmPred.overall rfPred.overall\nAccuracy       0.8145455       0.7890909      0.8043478\n\n\n\nBased on the highest accuracy, the random forest model did the best job fitting and predicting the data"
  }
]